node_lines:
- node_line_name: retrieve_node_line
  nodes:
    - node_type: query_expansion
      strategy:
        metrics: [retrieval_f1, retrieval_recall]
        top_k: 50
        retrieval_modules:
          - module_type: bm25
          - module_type: vectordb
            embedding_model: [openai, huggingface]
      modules:
        - module_type: query_decompose
          llm: [openai, huggingface]
          temperature: [0.2, 0.5, 1.0]
          top_p: 0.9
          max_token: 64
        - module_type: hyde
          llm: [openai, huggingface]
          temperature: 0.2
          top_p: 0.9
          max_token: 64
    - node_type: retrieval  # represents run_node function
      strategy:  # essential for every node
        metrics: [retrieval_f1, retrieval_recall]
      top_k: 50 # node param, which adapt to every module in this node.
      modules:
        - module_type: bm25
        - module_type: vectordb
          embedding_model: [openai, huggingface] # module param. experiment with these.
    - node_type: passage_reranker
      strategy:
        metrics: [retrieval_f1, retrieval_recall]
        speed_threshold: 5
      top_k: 10
      modules:
        - module_type: tart
        - module_type: monoT5
- node_line_name:
  nodes:
    - node_type: prompt_maker
      strategy:
        metrics: [bleu, rouge]
        generator_modules:
          - module_type: llama_index_llm
            llm: [openai]
            model_name: [gpt-3.5-turbo, gpt-4-1106-preview]
            temperature: 0.8
      modules:
        - module_type: fstring
          prompt: "This is a news dataset, crawled from finance news site. You need to make detailed question about finance news. Do not make questions that not relevant to economy or finance domain.\n{{retrieved_contents}}\n\nQ: {{query}}\nA:"
    - node_type: generation
      strategy:
        metrics: [bleu, rouge, kf1]
      modules:
        - module_type: llama_index_llm
          llm: [openai]
          model_name: [gpt-3.5-turbo, gpt-4-1106-preview]
          temperature: [0.5, 1.0, 1.5]
        - module_type: llama_index_llm
          llm: [huggingface]
          model_name: [mixtral, llama-2]
          top_p: [0.5, 0.8, 0.9]
