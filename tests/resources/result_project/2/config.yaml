vectordb:
  - name: chroma_default
    db_type: chroma
    client_type: persistent
    embedding_model: openai
    collection_name: openai
    path: ${PROJECT_DIR}/resources/chroma
    embedding_batch: 2
node_lines:
- node_line_name: pre_retrieve_node_line
  nodes:
    - node_type: query_expansion
      strategy:
        metrics: [retrieval_f1, retrieval_recall, retrieval_precision]
        speed_threshold: 10
        top_k: 10
        retrieval_modules:
          - module_type: bm25
          - module_type: vectordb
            vectordb: default
            embedding_batch: 2
      modules:
        - module_type: query_decompose
          generator_module_type: llama_index_llm
          llm: mock
          temperature: [0.2, 1.0]
        - module_type: hyde
          generator_module_type: llama_index_llm
          llm: mock
          max_token: 64
- node_line_name: retrieve_node_line
  nodes:
    - node_type: lexical_retrieval
      strategy:
        metrics: [retrieval_f1, retrieval_recall, retrieval_precision]
        speed_threshold: 10
      top_k: 10
      modules:
        - module_type: bm25
    - node_type: semantic_retrieval
      strategy:
        metrics: [ retrieval_f1, retrieval_recall, retrieval_precision ]
        speed_threshold: 10
      top_k: 10
      modules:
        - module_type: vectordb
          vectordb: default
          embedding_batch: 2
    - node_type: hybrid_retrieval
      strategy:
        metrics: [retrieval_f1, retrieval_recall, retrieval_precision]
        speed_threshold: 10
      top_k: 10
      modules:
        - module_type: hybrid_cc
    - node_type: passage_reranker
      strategy:
        metrics: [retrieval_f1, retrieval_recall, retrieval_precision]
        speed_threshold: 10
      top_k: 5
      modules:
        - module_type: tart
- node_line_name: post_retrieve_node_line  # Arbitrary node line name
  nodes:
    - node_type: prompt_maker
      strategy:
        metrics: [bleu, meteor, rouge]
        speed_threshold: 10
        generator_modules:
          - module_type: llama_index_llm
            llm: mock
      modules:
        - module_type: fstring
          prompt: ["Tell me something about the question: {query} \n\n {retrieved_contents}",
                   "Question: {query} \n Something to read: {retrieved_contents} \n What's your answer?"]
    - node_type: generator
      strategy:
        metrics: [bleu, meteor, rouge]
        speed_threshold: 10
      modules:
        - module_type: llama_index_llm
          llm: mock
