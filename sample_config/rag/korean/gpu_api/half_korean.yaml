node_lines:
- node_line_name: retrieve_node_line  # Arbitrary node line name
  nodes:
    - node_type: retrieval
      strategy:
        metrics: [ retrieval_f1, retrieval_recall, retrieval_precision,
                   retrieval_ndcg, retrieval_map, retrieval_mrr ]
        speed_threshold: 10
      top_k: 10
      modules:
        - module_type: bm25
          bm25_tokenizer: [ ko_kiwi, ko_okt, ko_kkma ]
        - module_type: vectordb
          vectordb: default
        - module_type: hybrid_rrf
          weight_range: (4,80)
        - module_type: hybrid_cc
          normalize_method: [ mm, tmm, z, dbsf ]
          weight_range: (0.0, 1.0)
          test_weight_size: 101
    - node_type: passage_augmenter
      strategy:
        metrics: [ retrieval_f1, retrieval_recall, retrieval_precision ]
        speed_threshold: 5
      top_k: 5
      embedding_model: openai
      modules:
        - module_type: pass_passage_augmenter
        - module_type: prev_next_augmenter
          mode: next
    - node_type: passage_reranker
      modules:
        - module_type: koreranker
        - module_type: flag_embedding_llm_reranker  # Requires enough GPU resources
        - module_type: cohere_reranker  # Set Environment Variable: COHERE_API_KEY
        - module_type: pass_reranker
      strategy:
        metrics: [ retrieval_recall, retrieval_precision, retrieval_map ]
      top_k: 3
    - node_type: passage_filter
      strategy:
        metrics: [ retrieval_f1, retrieval_recall, retrieval_precision ]
        speed_threshold: 5
      modules:
        - module_type: pass_passage_filter
        - module_type: similarity_threshold_cutoff
          threshold: 0.85
        - module_type: similarity_percentile_cutoff
          percentile: 0.6
        - module_type: threshold_cutoff
          threshold: 0.85
        - module_type: percentile_cutoff
          percentile: 0.6
    - node_type: passage_compressor
      strategy:
        metrics: [retrieval_token_f1, retrieval_token_recall, retrieval_token_precision]
        speed_threshold: 10
      modules:
        - module_type: pass_compressor
        - module_type: tree_summarize
          llm: openai
          model: gpt-4o-mini
          prompt: |
            여러 문맥 정보는 다음과 같습니다.\n
            ---------------------\n
            {context_str}\n
            ---------------------\n
            사전 지식이 아닌 여러 정보가 주어졌습니다,
            질문에 대답하세요.\n
            질문: {query_str}\n
            답변:
        - module_type: refine
          llm: openai
          model: gpt-4o-mini
          prompt: |
            원래 질문은 다음과 같습니다: {query_str}
            기존 답변은 다음과 같습니다: {existing_answer}
            아래에서 기존 답변을 정제할 수 있는 기회가 있습니다.
            (필요한 경우에만) 아래에 몇 가지 맥락을 추가하여 기존 답변을 정제할 수 있습니다.
            ------------
            {context_msg}
            ------------
            새로운 문맥이 주어지면 기존 답변을 수정하여 질문에 대한 답변을 정제합니다.
            맥락이 쓸모 없다면, 기존 답변을 그대로 답변하세요.
            정제된 답변:
        - module_type: longllmlingua
- node_line_name: post_retrieve_node_line  # Arbitrary node line name
  nodes:
    - node_type: prompt_maker
      strategy:
        metrics:
          - metric_name: bleu
          - metric_name: meteor
          - metric_name: rouge
          - metric_name: sem_score
            embedding_model: openai
        speed_threshold: 10
        generator_modules:
          - module_type: llama_index_llm
            llm: openai
            model: [gpt-4o-mini]
      modules:
        - module_type: fstring
          prompt: ["주어진 passage만을 이용하여 question에 따라 답하시오 passage: {retrieved_contents} \n\n Question: {query} \n\n Answer:"]
        - module_type: long_context_reorder
          prompt: ["주어진 passage만을 이용하여 question에 따라 답하시오 passage: {retrieved_contents} \n\n Question: {query} \n\n Answer:"]
    - node_type: generator
      strategy:
        metrics:
          - metric_name: rouge
          - embedding_model: openai
            metric_name: sem_score
          - metric_name: bert_score
            lang: ko
        speed_threshold: 10
      modules:
        - module_type: llama_index_llm
          llm: [openai]
          model: [gpt-4o-mini]
          temperature: [0.5, 1.0]
