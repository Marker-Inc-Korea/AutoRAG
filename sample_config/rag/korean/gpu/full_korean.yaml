node_lines:
- node_line_name: pre_retrieve_node_line  # Arbitrary node line name
  nodes:
    - node_type: query_expansion
      strategy:
        metrics: [retrieval_f1, retrieval_recall, retrieval_precision]
        speed_threshold: 10
        top_k: 10
        retrieval_modules:
          - module_type: bm25
            bm25_tokenizer: [ porter_stemmer, space, gpt2 ]
          - module_type: vectordb
            vectordb: default
      modules:
        - module_type: pass_query_expansion
        - module_type: hyde
          generator_module_type: llama_index_llm
          llm: openai
          model: [ gpt-4o-mini ]
          max_token: 64
          prompt: "질문에 답하기 위한 단락을 작성해 주세요."
        - module_type: multi_query_expansion
          generator_module_type: llama_index_llm
          llm: openai
          temperature: [ 0.2, 1.0 ]
          prompt: |
            당신은 인공지능 언어 모델 어시스턴트입니다.
            주어진 사용자 질문을 이용해 세 가지 버전의 새 질문을 생성하여 벡터 데이터베이스에서 관련 문서를 검색하는 것이 과제입니다.
            주어진 질문에 대한 다양한 관점을 생성함으로써 사용자가 거리 기반 유사도 검색의 한계를 극복할 수 있도록 돕는 것이 목표입니다.
            다음과 같은 대체 질문을 줄 바꿈으로 구분하여 제공하십시오.
            원래 질문: {query}
- node_line_name: retrieve_node_line  # Arbitrary node line name
  nodes:
    - node_type: retrieval
      strategy:
        metrics: [ retrieval_f1, retrieval_recall, retrieval_precision,
                   retrieval_ndcg, retrieval_map, retrieval_mrr ]
        speed_threshold: 10
      top_k: 10
      modules:
        - module_type: bm25
          bm25_tokenizer: [ ko_kiwi, ko_okt, ko_kkma ]
        - module_type: vectordb
          vectordb: default
        - module_type: hybrid_rrf
          weight_range: (4,80)
        - module_type: hybrid_cc
          normalize_method: [ mm, tmm, z, dbsf ]
          weight_range: (0.0, 1.0)
          test_weight_size: 101
    - node_type: passage_augmenter
      strategy:
        metrics: [ retrieval_f1, retrieval_recall, retrieval_precision ]
        speed_threshold: 5
      top_k: 5
      embedding_model: openai
      modules:
        - module_type: pass_passage_augmenter
        - module_type: prev_next_augmenter
          mode: next
    - node_type: passage_reranker
      modules:
        - module_type: koreranker
        - module_type: flag_embedding_llm_reranker  # Requires enough GPU resources
        - module_type: pass_reranker
      strategy:
        metrics: [ retrieval_recall, retrieval_precision, retrieval_map ]
      top_k: 3
    - node_type: passage_filter
      strategy:
        metrics: [ retrieval_f1, retrieval_recall, retrieval_precision ]
        speed_threshold: 5
      modules:
        - module_type: pass_passage_filter
        - module_type: similarity_threshold_cutoff
          threshold: 0.85
        - module_type: similarity_percentile_cutoff
          percentile: 0.6
        - module_type: threshold_cutoff
          threshold: 0.85
        - module_type: percentile_cutoff
          percentile: 0.6
    - node_type: passage_compressor
      strategy:
        metrics: [retrieval_token_f1, retrieval_token_recall, retrieval_token_precision]
        speed_threshold: 10
      modules:
        - module_type: pass_compressor
        - module_type: tree_summarize
          llm: openai
          model: gpt-4o-mini
          prompt: |
            여러 문맥 정보는 다음과 같습니다.\n
            ---------------------\n
            {context_str}\n
            ---------------------\n
            사전 지식이 아닌 여러 정보가 주어졌습니다,
            질문에 대답하세요.\n
            질문: {query_str}\n
            답변:
        - module_type: refine
          llm: openai
          model: gpt-4o-mini
          prompt: |
            원래 질문은 다음과 같습니다: {query_str}
            기존 답변은 다음과 같습니다: {existing_answer}
            아래에서 기존 답변을 정제할 수 있는 기회가 있습니다.
            (필요한 경우에만) 아래에 몇 가지 맥락을 추가하여 기존 답변을 정제할 수 있습니다.
            ------------
            {context_msg}
            ------------
            새로운 문맥이 주어지면 기존 답변을 수정하여 질문에 대한 답변을 정제합니다.
            맥락이 쓸모 없다면, 기존 답변을 그대로 답변하세요.
            정제된 답변:
        - module_type: longllmlingua
- node_line_name: post_retrieve_node_line  # Arbitrary node line name
  nodes:
    - node_type: prompt_maker
      strategy:
        metrics:
          - metric_name: bleu
          - metric_name: meteor
          - metric_name: rouge
          - metric_name: sem_score
            embedding_model: openai
        speed_threshold: 10
        generator_modules:
          - module_type: llama_index_llm
            llm: openai
            model: [gpt-4o-mini]
      modules:
        - module_type: fstring
          prompt: ["주어진 passage만을 이용하여 question에 따라 답하시오 passage: {retrieved_contents} \n\n Question: {query} \n\n Answer:"]
        - module_type: long_context_reorder
          prompt: ["주어진 passage만을 이용하여 question에 따라 답하시오 passage: {retrieved_contents} \n\n Question: {query} \n\n Answer:"]
    - node_type: generator
      strategy:
        metrics:
          - metric_name: rouge
          - embedding_model: openai
            metric_name: sem_score
          - metric_name: bert_score
            lang: ko
          - metric_name: g_eval  # LLM Judge Metric. Default Model: gpt-4-turbo
        speed_threshold: 10
      modules:
        - module_type: llama_index_llm
          llm: [openai]
          model: [gpt-4o-mini]
          temperature: [0.5, 1.0]
